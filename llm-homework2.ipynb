{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54eb4c39-35f5-4e1e-9fc3-516a9d34ee69",
   "metadata": {},
   "source": [
    "# Q1. Running Ollama with Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5126eca-24bf-45fe-8f2b-f9c61eb9afe5",
   "metadata": {},
   "source": [
    "#### [Q1 Answer] \n",
    "ollama version is 0.1.48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0144ec46-6d8d-40dc-8791-73f9b6cd9e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run ollama\n",
    "$ docker run -it \\\n",
    "    --rm \\\n",
    "    -v ollama:/root/.ollama \\\n",
    "    -p 11434:11434 \\\n",
    "    --name ollama \\\n",
    "    ollama/ollama\n",
    "\n",
    "# get container id\n",
    "$ docker ps\n",
    "\n",
    "# enter container\n",
    "$ docker exec -it 7a30839483c3 bash\n",
    "\n",
    "# get ollama version\n",
    "7a30839483c3$ ollama -v\n",
    "ollama version is 0.1.48"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fee0383-d1df-4038-8aec-03cf7d85aa5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Q2. Downloading an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff288967-a19d-4d20-8500-c714b1c3938b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### [Q2 Answer] \n",
    "{\"schemaVersion\":2,\"mediaType\":\"application/vnd.docker.distribution.manifest.v2+json\",\"config\":{\"mediaType\":\"application/vnd.docker.container.image.v1+json\",\"digest\":\"sha256:887433b89a901c156f7e6944442f3c9e57f3c55d6ed52042cbb7303aea994290\",\"size\":483},\"layers\":[{\"mediaType\":\"application/vnd.ollama.image.model\",\"digest\":\"sha256:c1864a5eb19305c40519da12cc543519e48a0697ecd30e15d5ac228644957d12\",\"size\":1678447520},{\"mediaType\":\"application/vnd.ollama.image.license\",\"digest\":\"sha256:097a36493f718248845233af1d3fefe7a303f864fae13bc31a3a9704229378ca\",\"size\":8433},{\"mediaType\":\"application/vnd.ollama.image.template\",\"digest\":\"sha256:109037bec39c0becc8221222ae23557559bc594290945a2c4221ab4f303b8871\",\"size\":136},{\"mediaType\":\"application/vnd.ollama.image.params\",\"digest\":\"sha256:22a838ceb7fb22755a3b0ae9b4eadde629d19be1f651f73efb8c6b4e2cd0eea0\",\"size\":84}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d93df94-c5f0-4d5a-ab52-ff9938e20d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull gemma in container\n",
    "7a30839483c3$ ollama pull gemma:2b\n",
    "\n",
    "# get gemma metadata\n",
    "7a30839483c3$ cat /root/.ollama/models/manifests/registry.ollama.ai/library/gemma/2b\n",
    "{\n",
    "\t\"schemaVersion\": 2,\n",
    "\t\"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n",
    "\t\"config\": {\n",
    "\t\t\"mediaType\": \"application/vnd.docker.container.image.v1+json\",\n",
    "\t\t\"digest\": \"sha256:887433b89a901c156f7e6944442f3c9e57f3c55d6ed52042cbb7303aea994290\",\n",
    "\t\t\"size\": 483\n",
    "\t},\n",
    "\t\"layers\": [\n",
    "\t\t{\n",
    "\t\t\t\"mediaType\": \"application/vnd.ollama.image.model\",\n",
    "\t\t\t\"digest\": \"sha256:c1864a5eb19305c40519da12cc543519e48a0697ecd30e15d5ac228644957d12\",\n",
    "\t\t\t\"size\": 1678447520\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t\"mediaType\": \"application/vnd.ollama.image.license\",\n",
    "\t\t\t\"digest\": \"sha256:097a36493f718248845233af1d3fefe7a303f864fae13bc31a3a9704229378ca\",\n",
    "\t\t\t\"size\": 8433\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t\"mediaType\": \"application/vnd.ollama.image.template\",\n",
    "\t\t\t\"digest\": \"sha256:109037bec39c0becc8221222ae23557559bc594290945a2c4221ab4f303b8871\",\n",
    "\t\t\t\"size\": 136\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t\"mediaType\": \"application/vnd.ollama.image.params\",\n",
    "\t\t\t\"digest\": \"sha256:22a838ceb7fb22755a3b0ae9b4eadde629d19be1f651f73efb8c6b4e2cd0eea0\",\n",
    "\t\t\t\"size\": 84\n",
    "\t\t}\n",
    "\t]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734cbf26-a825-4aa0-8fdb-558658f64c1e",
   "metadata": {},
   "source": [
    "# Q3. Running the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884d2f2b-a2af-40da-9776-e0b1013f72f2",
   "metadata": {},
   "source": [
    "#### [Q3 Answer] \n",
    " To solve this, we simply perform the multiplication of the two numbers provided:\n",
    "\n",
    "\n",
    "```plaintext\n",
    "\n",
    "  10\n",
    "\n",
    "x 10\n",
    "\n",
    "———\n",
    "\n",
    "  100\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Therefore, `1 startMultiplication` equals `100`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1bbee7-44ee-4f6a-91da-dbd449f51d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run llm\n",
    "7a30839483c3$ ollama run phi3\n",
    "\n",
    "# given prompt\n",
    ">>> 10 * 10\n",
    " To solve this, we simply perform the multiplication of the two numbers provided:\n",
    "\n",
    "\n",
    "```plaintext\n",
    "\n",
    "  10\n",
    "\n",
    "x 10\n",
    "\n",
    "———\n",
    "\n",
    "  100\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Therefore, `1 startMultiplication` equals `100`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d52d087-02c0-47c3-9622-8ab40964f719",
   "metadata": {},
   "source": [
    "# Q4. Donwloading the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b39a84b-f674-46ab-b611-b6cfb88ea566",
   "metadata": {},
   "source": [
    "#### [Q4 Answer] \n",
    "1.7G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1e599a-5abf-4bfe-ac4c-aef94c1c844e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create local ollama folder\n",
    "$ mkdir /home/jupyter/llm/ollama_files\n",
    "\n",
    "# run ollama container again with mapping to local folder\n",
    "$ docker run -it \\\n",
    "    --rm \\\n",
    "    -v /home/jupyter/llm/ollama_files:/root/.ollama \\\n",
    "    -p 11434:11434 \\\n",
    "    --name ollama \\\n",
    "    ollama/ollama\n",
    "\n",
    "# pulla gemma model in new container\n",
    "$ docker exec -it ollama ollama pull gemma:2b \n",
    "\n",
    "# enter container\n",
    "$ docker exec -it ollama bash\n",
    "\n",
    "# get models folder size (from container)\n",
    "da2271fff893$ du -h /root/.ollama/models\n",
    "8.0K    models/manifests/registry.ollama.ai/library/gemma\n",
    "12K     models/manifests/registry.ollama.ai/library\n",
    "16K     models/manifests/registry.ollama.ai\n",
    "20K     models/manifests\n",
    "1.6G    models/blobs\n",
    "1.6G    models\n",
    "\n",
    "# get models folder size (from local)\n",
    "$ du -h /home/jupyter/llm/ollama_files\n",
    "8.0K    /home/jupyter/llm/ollama_files/models/manifests/registry.ollama.ai/library/gemma\n",
    "12K     /home/jupyter/llm/ollama_files/models/manifests/registry.ollama.ai/library\n",
    "16K     /home/jupyter/llm/ollama_files/models/manifests/registry.ollama.ai\n",
    "20K     /home/jupyter/llm/ollama_files/models/manifests\n",
    "1.6G    /home/jupyter/llm/ollama_files/models/blobs\n",
    "1.6G    /home/jupyter/llm/ollama_files/models\n",
    "1.6G    /home/jupyter/llm/ollama_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92c0065-3513-436b-8bca-a053e1741e0d",
   "metadata": {},
   "source": [
    "# Q5. Adding the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec500049-d89b-4c58-b97a-297d98f96a4d",
   "metadata": {},
   "source": [
    "#### [Q5 Answer] \n",
    "./ollama_files /root/.ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d468869-9c39-41ec-a7c0-8a3b9637f21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dockerfile\n",
    "'''\n",
    "FROM ollama/ollama\n",
    "COPY ./ollama_files /root/.ollama\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5446da51-8b12-415e-b85b-8dd4528ba47c",
   "metadata": {},
   "source": [
    "# Q6. Serving it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53d717f-bf6a-48c0-8de6-bd2de5e390e7",
   "metadata": {},
   "source": [
    "#### [Q6 Answer] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575ce009-97ae-4ad0-abc6-b2a81eb7f415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build it\n",
    "$ cd /home/jupyter/llm/\n",
    "$ docker build -t ollama-gemma2b .\n",
    "\n",
    "# run it\n",
    "$ docker run -it --rm -p 11434:11434 --name ollama ollama-gemma2b\n",
    "\n",
    "# enter container\n",
    "$ docker exec -it ollama bash\n",
    "\n",
    "# check models\n",
    "b7d183c5b0d7$ du -h /root/.ollama/models\n",
    "8.0K    /root/.ollama/models/manifests/registry.ollama.ai/library/gemma\n",
    "12K     /root/.ollama/models/manifests/registry.ollama.ai/library\n",
    "16K     /root/.ollama/models/manifests/registry.ollama.ai\n",
    "20K     /root/.ollama/models/manifests\n",
    "1.6G    /root/.ollama/models/blobs\n",
    "1.6G    /root/.ollama/models\n",
    "\n",
    "'''\n",
    "curl http://localhost:11434/api/generate -d '{\n",
    "        \"model\": \"phi3\",\n",
    "        \"prompt\": \"10*10\",\n",
    "        \"stream\": true,\n",
    "        \"options\": {\n",
    "            \"seed\": 123,\n",
    "            \"top_k\": 20,\n",
    "            \"top_p\": 0.9,\n",
    "            \"temperature\": 0\n",
    "        }\n",
    "    }'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85dce4ef-5a96-4bda-831c-42921a0113f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The basic formula to calculate mechanical energy (which is a combination of potential and kinetic energy) in classical mechanics, assuming no non-conservative forces like friction are at work, can be expressed as:\n",
      "\n",
      "E = PE + KE\n",
      "\n",
      "where E represents the total mechanical energy, PE stands for gravitational potential energy, and KE is kinetic energy. The formulas for each of these components in a system where only gravity acts on an object near Earth's surface are given by:\n",
      "\n",
      "PE = m * g * h\n",
      "KE = 0.5 * m * v^2\n",
      "\n",
      "Here, 'm' represents the mass of the object, 'g' is the acceleration due to gravity (approximately 9.81 m/s^2 near Earth's surface), 'h' stands for height above a reference point, and 'v' is velocity. So if you want just one formula that encompasses both potential and kinetic energy in such scenarios:\n",
      "\n",
      "E = m * g * h + 0.5 * m * v^2\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    " \n",
    "key_filepath = \"/home/jupyter/llm-camp-homework/key\"\n",
    "key = open(key_filepath, 'r', encoding='utf-8').read() if key_filepath else None\n",
    "os.environ[\"OPENAI_API_KEY\"] = key\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = \"http://localhost:11434/v1/\"\n",
    ")\n",
    "\n",
    "def llm(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model       = 'phi3', \n",
    "        messages    = [{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature = 0.0\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "prompt = \"What's the formula for energy?\"\n",
    "response = llm(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26710d89-4d53-4525-8256-3f1f373a7662",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "        You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "        Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "        QUESTION: {question}\n",
    "\n",
    "        CONTEXT:\n",
    "        {context}\n",
    "    \"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt\n",
    "\n",
    "#prompt = build_prompt(query, search_results)\n",
    "#print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcbf5652-94e3-4a68-b7af-1963e1122ed0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I'm sorry, but it seems like there is no context provided regarding a formula for energy in this FAQ database. The questions mentioned are related to machine learning concepts such as linear regression, matrix multiplication and target variables used within specific projects or reports. If you have any other question about these topics, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "key_filepath = \"/home/jupyter/llm-camp-homework/key\"\n",
    "key = open(key_filepath, 'r', encoding='utf-8').read() if key_filepath else None\n",
    "os.environ[\"OPENAI_API_KEY\"] = key\n",
    "client = OpenAI(\n",
    "    base_url = \"http://localhost:11434/v1/\"\n",
    ")\n",
    "\n",
    "def llm(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model       = 'phi3', \n",
    "        messages    = [{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature = 0.0\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "response = llm(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f3c128b-4470-48bb-b861-20cc6c0458f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-07-09 13:56:36--  https://raw.githubusercontent.com/alexeygrigorev/minsearch/main/minsearch.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3832 (3.7K) [text/plain]\n",
      "Saving to: ‘minsearch.py’\n",
      "\n",
      "minsearch.py        100%[===================>]   3.74K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-07-09 13:56:36 (19.9 MB/s) - ‘minsearch.py’ saved [3832/3832]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!rm -f minsearch.py\n",
    "!wget https://raw.githubusercontent.com/alexeygrigorev/minsearch/main/minsearch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43960ddc-f396-435a-bd7f-57d129f0eec6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.Index at 0x7f7ccf0c5330>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests \n",
    "import minsearch\n",
    "\n",
    "docs_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/01-intro/documents.json?raw=1'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)\n",
    "\n",
    "index = minsearch.Index(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bfc0deb-3ee7-44b2-b9d5-0a0157c9c98e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        #filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1273fab-6a2d-4428-8de3-1f09eb4ab388",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a17f25aaa0174da8bf340868e5799ccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device device because they were offloaded to the disk and cpu.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5dc8dfa1-5954-43c8-8508-bfd0cff66a02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': \"Yes, even if you don't register, you're still eligible to submit the homeworks.\\nBe aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\", 'section': 'General course-related questions', 'question': 'Course - Can I still join the course after the start date?', 'course': 'data-engineering-zoomcamp'}, {'text': 'Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\\nYou can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.', 'section': 'General course-related questions', 'question': 'Course - Can I follow the course after it finishes?', 'course': 'data-engineering-zoomcamp'}, {'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon’t forget to register in DataTalks.Club's Slack and join the channel.\", 'section': 'General course-related questions', 'question': 'Course - When will the course start?', 'course': 'data-engineering-zoomcamp'}, {'text': 'You can start by installing and setting up all the dependencies and requirements:\\nGoogle cloud account\\nGoogle Cloud SDK\\nPython 3 (installed with Anaconda)\\nTerraform\\nGit\\nLook over the prerequisites and syllabus to see if you are comfortable with these subjects.', 'section': 'General course-related questions', 'question': 'Course - What can I do before the course starts?', 'course': 'data-engineering-zoomcamp'}, {'text': 'Star the repo! Share it with friends if you find it useful ❣️\\nCreate a PR if you see you can improve the text or the structure of the repository.', 'section': 'General course-related questions', 'question': 'How can we contribute to the course?', 'course': 'data-engineering-zoomcamp'}]\n"
     ]
    }
   ],
   "source": [
    "query = \"I just discovered the course. Can I still join it?\"\n",
    "search_results = search(query)\n",
    "print(search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2a1e331-5723-48aa-8b91-310549bfce46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "key_filepath = \"/home/jupyter/llm-camp-homework/key\"\n",
    "key = open(key_filepath, 'r', encoding='utf-8').read() if key_filepath else None\n",
    "os.environ[\"OPENAI_API_KEY\"] = key\n",
    "client = OpenAI(\n",
    "    base_url = \"http://localhost:11434/v1/\"\n",
    ")\n",
    "\n",
    "def llm(prompt):\n",
    "    # openai\n",
    "    response = client.chat.completions.create(\n",
    "        model       = 'phi3', \n",
    "        messages    = [{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature = 0.0\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "    \n",
    "    '''\n",
    "    # customized\n",
    "    if generate_params is None:\n",
    "        generate_params = {}\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    #input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_length=generate_params.get(\"max_length\", 100),\n",
    "        num_beams=generate_params.get(\"num_beams\", 5),\n",
    "        do_sample=generate_params.get(\"do_sample\", False),\n",
    "        temperature=generate_params.get(\"temperature\", 1.0),\n",
    "        top_k=generate_params.get(\"top_k\", 50),\n",
    "        top_p=generate_params.get(\"top_p\", 0.95),\n",
    "    )\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return result\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a59f4c2-86e5-4ffb-84dd-3c0f684986c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The homework URL in the context provided is not explicitly mentioned as being for energy calculations specifically; however, based on general knowledge about course-related questions regarding repositories:\n",
      "\n",
      "The formula for calculating energy (E) can be given by E = mgh or E = 1/2 mv^2 + mgh depending on whether you're considering gravitational potential energy in a vertical system and kinetic energy. Here, 'm' stands for mass, 'g' is the acceleration due to gravity (approximately 9.81 m/s² near Earth’s surface), 'h' represents height or displacement within a gravitational field, and 'v' denotes velocity of an object with respect to its surr0672345_energy-formula\n",
      "tion in the vertical direction (ignoring air resistance). These formulas are fundamental concepts that might be covered as part of coursework related to physics.\n"
     ]
    }
   ],
   "source": [
    "query = \"What's the formula for energy?\"\n",
    "search_results = search(query)\n",
    "prompt = build_prompt(query, search_results)\n",
    "answer = llm(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2f0617b-a7de-4703-8008-6477fcc165f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "836\n"
     ]
    }
   ],
   "source": [
    "s = \"\"\"\n",
    " The homework URL in the context provided is not explicitly mentioned as being for energy calculations specifically; however, based on general knowledge about course-related questions regarding repositories:\n",
    "\n",
    "The formula for calculating energy (E) can be given by E = mgh or E = 1/2 mv^2 + mgh depending on whether you're considering gravitational potential energy in a vertical system and kinetic energy. Here, 'm' stands for mass, 'g' is the acceleration due to gravity (approximately 9.81 m/s² near Earth’s surface), 'h' represents height or displacement within a gravitational field, and 'v' denotes velocity of an object with respect to its surr0672345_energy-formula\n",
    "tion in the vertical direction (ignoring air resistance). These formulas are fundamental concepts that might be covered as part of coursework related to physics.\n",
    "\"\"\"\n",
    "print(len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaee9d4-0d12-442f-a912-e942dd494218",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m123"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
